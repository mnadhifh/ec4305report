{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6151872",
   "metadata": {},
   "source": [
    "# Feature Engineering: Stationarity Testing\n",
    "\n",
    "This notebook tests the stationarity of price and production time series data using the Augmented Dickey-Fuller (ADF) test. The pipeline is designed to be modular and reusable for adding new variables from additional CSV files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd60b481",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c33b4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2d45af",
   "metadata": {},
   "source": [
    "## 2. Load and Filter Time Series Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7dc262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define date range: January 1, 1990 to June 2025\n",
    "start_date = datetime(1990, 1, 1)\n",
    "end_date = datetime(2025, 6, 30)\n",
    "\n",
    "# Define the CSV files and their corresponding column names\n",
    "# Format: {'csv_filename': 'column_name_in_csv'}\n",
    "csv_config = {\n",
    "    'data/beefprice.csv': 'PBEEFUSDM',\n",
    "    'data/cornprice.csv': 'PMAIZMTUSDM',\n",
    "    'data/soybeanprice.csv': 'PSMEAUSDM',\n",
    "    'data/brentoilprice.csv': 'POILBREUSDM',\n",
    "    'data/lambprice.csv': 'PLAMBUSDM',\n",
    "    'data/porkprice.csv': 'PPORKUSDM',\n",
    "    'data/poultryprice.csv': 'PPOULTUSDM',\n",
    "    'data/bioethanolprod.csv': 'Bioethanol production',  # Special handling for this file\n",
    "}\n",
    "\n",
    "# Load and filter data\n",
    "def load_and_filter_data(csv_path, date_col='observation_date', value_col='PRICE'):\n",
    "    \"\"\"\n",
    "    Load CSV file and filter to date range [1990-01-01, 2025-06-30]\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Parse date column\n",
    "    df[date_col] = pd.to_datetime(df[date_col])\n",
    "    \n",
    "    # Filter by date range\n",
    "    df_filtered = df[(df[date_col] >= start_date) & (df[date_col] <= end_date)].copy()\n",
    "    \n",
    "    # Sort by date\n",
    "    df_filtered = df_filtered.sort_values(date_col).reset_index(drop=True)\n",
    "    \n",
    "    # Extract the value column\n",
    "    return df_filtered, df_filtered[value_col].values\n",
    "\n",
    "# Test loading one file to check structure\n",
    "sample_df, _ = load_and_filter_data('data/beefprice.csv', value_col='PBEEFUSDM')\n",
    "print(\"Sample data structure:\")\n",
    "print(sample_df.head())\n",
    "print(f\"\\nDate range: {sample_df['observation_date'].min()} to {sample_df['observation_date'].max()}\")\n",
    "print(f\"Number of observations: {len(sample_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808034e3",
   "metadata": {},
   "source": [
    "## 3. Extract Variables and Store in Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dd279b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all variables and store in a dictionary\n",
    "# IMPORTANT: Consolidate to 1 observation per MONTH-YEAR (ignore day of month)\n",
    "variables = {}\n",
    "\n",
    "for csv_path, value_col in csv_config.items():\n",
    "    try:\n",
    "        # Special handling for bioethanol file with semicolon separator\n",
    "        if 'bioethanol' in csv_path.lower():\n",
    "            df = pd.read_csv(csv_path, sep=';')\n",
    "            df['Date'] = pd.to_datetime(df['Date'])\n",
    "            df_filtered = df[(df['Date'] >= start_date) & (df['Date'] <= end_date)].copy()\n",
    "            df_filtered = df_filtered.sort_values('Date').reset_index(drop=True)\n",
    "        else:\n",
    "            df_filtered, _ = load_and_filter_data(csv_path, value_col=value_col)\n",
    "        \n",
    "        # Group by month-year and take the last value of each month\n",
    "        # This ensures 1 observation per month regardless of the day of month\n",
    "        if 'bioethanol' in csv_path.lower():\n",
    "            date_col = 'Date'\n",
    "            value_col_name = 'Bioethanol production'\n",
    "        else:\n",
    "            date_col = 'observation_date'\n",
    "            value_col_name = value_col\n",
    "        \n",
    "        df_filtered['year_month'] = df_filtered[date_col].dt.to_period('M')\n",
    "        df_monthly = df_filtered.groupby('year_month')[value_col_name].last().reset_index()\n",
    "        df_monthly['date'] = df_monthly['year_month'].dt.to_timestamp()\n",
    "        \n",
    "        values = df_monthly[value_col_name].values\n",
    "        dates = df_monthly['date'].values\n",
    "        \n",
    "        # Use the CSV filename (without .csv) as the variable name\n",
    "        var_name = csv_path.split('/')[-1].replace('.csv', '')\n",
    "        \n",
    "        variables[var_name] = {\n",
    "            'data': values,\n",
    "            'dates': dates,\n",
    "            'length': len(values),\n",
    "            'csv_path': csv_path\n",
    "        }\n",
    "        \n",
    "        print(f\"âœ“ Loaded {var_name}: {len(values)} observations (1 per month-year)\")\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Error loading {csv_path}: {e}\")\n",
    "\n",
    "print(f\"\\nTotal variables loaded: {len(variables)}\")\n",
    "print(f\"Expected: 426 observations per variable (1990-01 to 2025-06)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccd1d3d",
   "metadata": {},
   "source": [
    "## 4. Augmented Dickey-Fuller Test Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d330afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_adf_test(timeseries, name):\n",
    "    \"\"\"\n",
    "    Perform Augmented Dickey-Fuller test on a time series.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    timeseries : array-like\n",
    "        The time series data to test\n",
    "    name : str\n",
    "        Name of the variable for reporting\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionary containing ADF test results\n",
    "    \"\"\"\n",
    "    # Remove NaN values\n",
    "    timeseries = timeseries[~np.isnan(timeseries)]\n",
    "    \n",
    "    # Perform ADF test\n",
    "    result = adfuller(timeseries, autolag='AIC')\n",
    "    \n",
    "    # Extract results\n",
    "    adf_result = {\n",
    "        'Variable': name,\n",
    "        'ADF Statistic': result[0],\n",
    "        'P-Value': result[1],\n",
    "        'Lags Used': result[2],\n",
    "        'Observations': result[3],\n",
    "        'Critical Value (1%)': result[4]['1%'],\n",
    "        'Critical Value (5%)': result[4]['5%'],\n",
    "        'Critical Value (10%)': result[4]['10%'],\n",
    "        'Stationary (Î±=0.05)': 'Yes' if result[1] < 0.05 else 'No'\n",
    "    }\n",
    "    \n",
    "    return adf_result\n",
    "\n",
    "# Test the function on one variable\n",
    "print(\"Example ADF Test:\")\n",
    "sample_result = perform_adf_test(variables['beefprice']['data'], 'beefprice')\n",
    "for key, value in sample_result.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627375ea",
   "metadata": {},
   "source": [
    "## 5. Run ADF Tests on All Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856217cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run ADF tests on all variables\n",
    "adf_results = []\n",
    "\n",
    "for var_name, var_data in variables.items():\n",
    "    result = perform_adf_test(var_data['data'], var_name)\n",
    "    adf_results.append(result)\n",
    "    print(f\"âœ“ ADF test completed for {var_name}\")\n",
    "\n",
    "# Create a DataFrame for easy viewing\n",
    "adf_df = pd.DataFrame(adf_results)\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"ADF TEST RESULTS SUMMARY\")\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba391fd4",
   "metadata": {},
   "source": [
    "## 6. Display Stationarity Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c075677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display comprehensive results table\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "print(adf_df.to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "\n",
    "# Summary statistics\n",
    "stationary_vars = adf_df[adf_df['Stationary (Î±=0.05)'] == 'Yes']\n",
    "non_stationary_vars = adf_df[adf_df['Stationary (Î±=0.05)'] == 'No']\n",
    "\n",
    "print(f\"\\nSTATIONARITY SUMMARY (Significance Level Î±=0.05):\")\n",
    "print(f\"Stationary Variables ({len(stationary_vars)}): {', '.join(stationary_vars['Variable'].tolist())}\")\n",
    "print(f\"Non-Stationary Variables ({len(non_stationary_vars)}): {', '.join(non_stationary_vars['Variable'].tolist())}\")\n",
    "print(f\"\\nTotal Variables Tested: {len(adf_df)}\")\n",
    "print(f\"Percentage Stationary: {(len(stationary_vars)/len(adf_df)*100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310bb51f",
   "metadata": {},
   "source": [
    "## 7. Fractional Differencing (Lopez de Prado, 2018)\n",
    "\n",
    "Apply fixed-width window fractional differencing only to non-stationary series. Stationary series are kept in levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5aaf2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ffd_weights(d, tau=1e-5):\n",
    "    \"\"\"\n",
    "    Compute weights for fixed-width window fractional differencing.\n",
    "    Uses the binomial series expansion: w_k = -w_{k-1} * (d - k + 1) / k\n",
    "    \"\"\"\n",
    "    weights = [1.0]\n",
    "    for k in range(1, 10000):\n",
    "        w = -weights[-1] * (d - k + 1) / k\n",
    "        if abs(w) < tau:\n",
    "            break\n",
    "        weights.append(w)\n",
    "    return np.array(weights)\n",
    "\n",
    "def apply_ffd_to_series(series, d, weights):\n",
    "    \"\"\"\n",
    "    Apply fractional differencing to a series using precomputed weights.\n",
    "    First (width-1) values will be NaN.\n",
    "    \"\"\"\n",
    "    if np.all(np.isnan(series)):\n",
    "        return series\n",
    "    \n",
    "    width = len(weights)\n",
    "    ffd = np.full_like(series, np.nan)\n",
    "    \n",
    "    for i in range(width - 1, len(series)):\n",
    "        window = series[i - width + 1:i + 1]\n",
    "        ffd[i] = np.dot(weights, window)\n",
    "    \n",
    "    return ffd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ee73ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fracdiff_with_existing_adf(variables, adf_df, alpha=0.05, d_grid=None, \n",
    "                               tau=1e-5, log_transform=True, columns=None):\n",
    "    \"\"\"\n",
    "    Create aligned DataFrame with optimal FFD of non-stationary series.\n",
    "    Tests on the RAW log-transformed series for accuracy.\n",
    "    \"\"\"\n",
    "    if d_grid is None:\n",
    "        d_grid = np.linspace(0, 1, 201)  # Finer grid for better accuracy\n",
    "    \n",
    "    if columns is None:\n",
    "        columns = list(variables.keys())\n",
    "    \n",
    "    # Build initial DataFrame with all dates\n",
    "    all_dates = set()\n",
    "    for var_name in columns:\n",
    "        if var_name in variables:\n",
    "            all_dates.update(variables[var_name]['dates'])\n",
    "    \n",
    "    all_dates = sorted(all_dates)\n",
    "    df_aligned = pd.DataFrame({'date': all_dates})\n",
    "    df_aligned.set_index('date', inplace=True)\n",
    "    \n",
    "    # Add each variable and find optimal d\n",
    "    summary_rows = []\n",
    "    \n",
    "    for var_name in columns:\n",
    "        if var_name not in variables:\n",
    "            continue\n",
    "        \n",
    "        var_data = variables[var_name]\n",
    "        dates = var_data['dates']\n",
    "        series = var_data['data'].astype(float)\n",
    "        \n",
    "        # Apply log transform if requested\n",
    "        if log_transform:\n",
    "            series = np.log(series + 1e-8)  # Avoid log(0)\n",
    "        \n",
    "        # Create temp series on original dates\n",
    "        temp_series = pd.Series(series, index=dates)\n",
    "        \n",
    "        # Align to full date range (forward fill)\n",
    "        aligned_series_raw = temp_series.reindex(df_aligned.index)\n",
    "        aligned_series_raw = aligned_series_raw.fillna(method='ffill')\n",
    "        aligned_series_np = aligned_series_raw.values\n",
    "        \n",
    "        # Determine if original raw series is stationary\n",
    "        series_clean = series[~np.isnan(series)]\n",
    "        adf_result_orig = adfuller(series_clean, autolag='AIC', regression='c')\n",
    "        is_stationary_orig = adf_result_orig[1] < alpha\n",
    "        \n",
    "        # Find optimal d if not stationary\n",
    "        # TEST ON THE RAW (UNALIGNED) SERIES first for accurate d determination\n",
    "        if is_stationary_orig:\n",
    "            optimal_d = 0\n",
    "            adf_pval_final = adf_result_orig[1]\n",
    "            lags_used = adf_result_orig[2]\n",
    "            notes = \"Already stationary (d=0)\"\n",
    "            # For stationary series, use the aligned raw data\n",
    "            final_series_np = aligned_series_np.copy()\n",
    "        else:\n",
    "            # Find optimal d by testing on RAW (not forward-filled) series\n",
    "            optimal_d = None\n",
    "            for d in d_grid:\n",
    "                weights = compute_ffd_weights(d, tau=tau)\n",
    "                ffd_raw = apply_ffd_to_series(series, d, weights)  # Apply to raw log-transformed series\n",
    "                ffd_clean = ffd_raw[~np.isnan(ffd_raw)]\n",
    "                \n",
    "                if len(ffd_clean) < 3:\n",
    "                    continue\n",
    "                \n",
    "                result = adfuller(ffd_clean, autolag='AIC', regression='c')\n",
    "                if result[1] < alpha:\n",
    "                    optimal_d = d\n",
    "                    adf_pval_final = result[1]\n",
    "                    lags_used = result[2]\n",
    "                    break  # Return first d that passes (minimum)\n",
    "            \n",
    "            # If no d found in initial grid, use extended search up to 1.0\n",
    "            if optimal_d is None:\n",
    "                extended_grid = np.linspace(d_grid[-1], 1.0, 100)\n",
    "                for d in extended_grid[1:]:  # Skip the first point (already tested)\n",
    "                    weights = compute_ffd_weights(d, tau=tau)\n",
    "                    ffd_raw = apply_ffd_to_series(series, d, weights)  # Test on raw\n",
    "                    ffd_clean = ffd_raw[~np.isnan(ffd_raw)]\n",
    "                    \n",
    "                    if len(ffd_clean) < 3:\n",
    "                        continue\n",
    "                    \n",
    "                    result = adfuller(ffd_clean, autolag='AIC', regression='c')\n",
    "                    if result[1] < alpha:\n",
    "                        optimal_d = d\n",
    "                        adf_pval_final = result[1]\n",
    "                        lags_used = result[2]\n",
    "                        break\n",
    "            \n",
    "            if optimal_d is None:\n",
    "                # If still no d found, use max d in grid\n",
    "                optimal_d = d_grid[-1]\n",
    "                notes = f\"No d in grid passed ADF; used max d={optimal_d:.3f}\"\n",
    "            else:\n",
    "                notes = f\"Found optimal d={optimal_d:.3f}\"\n",
    "            \n",
    "            # Now apply the FFD to ALIGNED series for storage in DataFrame\n",
    "            weights = compute_ffd_weights(optimal_d, tau=tau)\n",
    "            final_series_np = apply_ffd_to_series(aligned_series_np, optimal_d, weights)\n",
    "        \n",
    "        # Add final series to aligned DataFrame\n",
    "        df_aligned[var_name] = final_series_np\n",
    "        \n",
    "        # Record summary\n",
    "        summary_rows.append({\n",
    "            'Variable': var_name,\n",
    "            'Original Stationary': 'Yes' if is_stationary_orig else 'No',\n",
    "            'Optimal d': optimal_d if optimal_d is not None else np.nan,\n",
    "            'ADF p-value (final)': adf_pval_final,\n",
    "            'Lags Used': lags_used,\n",
    "            'Notes': notes\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_rows)\n",
    "    \n",
    "    return df_aligned, summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777aefad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the fractional differencing pipeline\n",
    "print(\"=\"*100)\n",
    "print(\"FRACTIONAL DIFFERENCING PIPELINE\")\n",
    "print(\"=\"*100)\n",
    "d_grid = np.linspace(0, 1, 201)  # 201 points for finer granularity\n",
    "features_df, summary_df = fracdiff_with_existing_adf(\n",
    "    variables=variables,\n",
    "    adf_df=adf_df,\n",
    "    alpha=0.05,\n",
    "    d_grid=d_grid,\n",
    "    tau=0.01,  # Optimized: 12-month window, minimal data loss\n",
    "    log_transform=True,\n",
    "    columns=list(variables.keys())\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"FRACTIONAL DIFFERENCING SUMMARY\")\n",
    "print(\"=\"*100)\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"FINAL FEATURES DATAFRAME\")\n",
    "print(\"=\"*100)\n",
    "print(f\"Shape: {features_df.shape}\")\n",
    "print(f\"Date range: {features_df.index.min()} to {features_df.index.max()}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(features_df.head())\n",
    "print(\"\\nData types:\")\n",
    "print(features_df.dtypes)\n",
    "print(\"\\nMissing values:\")\n",
    "print(features_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e636ec",
   "metadata": {},
   "source": [
    "## 8. Complete Workflow Summary\n",
    "\n",
    "The notebook implements a full feature engineering pipeline:\n",
    "1. **Load & Filter**: Extract data from Jan 1990 to Jun 2025, 1 observation per month-year\n",
    "2. **Stationarity Test**: ADF test to identify non-stationary series\n",
    "3. **Fractional Differencing**: Apply Lopez de Prado's FFD only to non-stationary series\n",
    "4. **Alignment & Output**: Create a unified modeling DataFrame with all series properly transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5d78cb",
   "metadata": {},
   "source": [
    "## 9. Data Visualization and Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e3601c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display detailed data inspection\n",
    "print(\"=\"*100)\n",
    "print(\"DETAILED DATA INSPECTION\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Get the last 10 non-NaN rows for each column\n",
    "print(\"\\n1. LAST 10 ROWS WITH DATA (showing end of time series):\")\n",
    "print(\"=\"*100)\n",
    "print(features_df.tail(10))\n",
    "\n",
    "print(\"\\n\\n2. SUMMARY STATISTICS:\")\n",
    "print(\"=\"*100)\n",
    "print(features_df.describe())\n",
    "\n",
    "print(\"\\n\\n3. DATA AVAILABILITY PER VARIABLE:\")\n",
    "print(\"=\"*100)\n",
    "for col in features_df.columns:\n",
    "    valid_count = features_df[col].notna().sum()\n",
    "    pct_valid = (valid_count / len(features_df)) * 100\n",
    "    print(f\"{col:20s}: {valid_count:4d} / {len(features_df)} rows ({pct_valid:5.1f}% available)\")\n",
    "\n",
    "print(\"\\n\\n4. SAMPLE OF COMPLETE DATA (rows with all values present):\")\n",
    "print(\"=\"*100)\n",
    "complete_rows = features_df.dropna()\n",
    "if len(complete_rows) > 0:\n",
    "    print(f\"Found {len(complete_rows)} complete rows (no missing values)\")\n",
    "    print(\"\\nLast 5 complete rows:\")\n",
    "    print(complete_rows.tail(5))\n",
    "else:\n",
    "    print(\"No rows with all values present (expected due to staggered data start dates)\")\n",
    "    print(\"\\nShowing rows with most data available:\")\n",
    "    available_count = features_df.notna().sum(axis=1)\n",
    "    top_rows_idx = available_count.nlargest(5).index\n",
    "    print(features_df.loc[top_rows_idx])\n",
    "\n",
    "print(\"\\n\\n5. CORRELATION MATRIX (non-stationary series after FD + stationary in levels):\")\n",
    "print(\"=\"*100)\n",
    "# Drop rows with too many NaNs for correlation\n",
    "corr_df = features_df.dropna()\n",
    "if len(corr_df) > 0:\n",
    "    correlation_matrix = corr_df.corr()\n",
    "    print(correlation_matrix)\n",
    "    \n",
    "    # Find highest correlations (excluding diagonal)\n",
    "    print(\"\\n\\nTOP CORRELATIONS (excluding self-correlation):\")\n",
    "    print(\"=\"*100)\n",
    "    corr_pairs = []\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i+1, len(correlation_matrix.columns)):\n",
    "            col1 = correlation_matrix.columns[i]\n",
    "            col2 = correlation_matrix.columns[j]\n",
    "            corr_val = correlation_matrix.iloc[i, j]\n",
    "            corr_pairs.append((col1, col2, corr_val))\n",
    "    \n",
    "    corr_pairs.sort(key=lambda x: abs(x[2]), reverse=True)\n",
    "    for col1, col2, corr_val in corr_pairs[:10]:\n",
    "        print(f\"{col1:20s} <-> {col2:20s}: {corr_val:7.3f}\")\n",
    "else:\n",
    "    print(\"Not enough complete data for correlation analysis\")\n",
    "\n",
    "print(\"\\n\\n6. TRANSFORMATION SUMMARY:\")\n",
    "print(\"=\"*100)\n",
    "for idx, row in summary_df.iterrows():\n",
    "    var = row['Variable']\n",
    "    d_val = row['Optimal d']\n",
    "    stationary = row['Original Stationary']\n",
    "    pval = row['ADF p-value (final)']\n",
    "    \n",
    "    if d_val == 0:\n",
    "        transform_type = \"Level (no transformation)\"\n",
    "    else:\n",
    "        transform_type = f\"Fractionally Differenced (d={d_val:.3f})\"\n",
    "    \n",
    "    print(f\"{var:20s}: {transform_type:40s} | P-value: {pval:.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"READY FOR MODELING!\")\n",
    "print(\"=\"*100)\n",
    "print(f\"\\nYour features_df DataFrame has:\")\n",
    "print(f\"  - {features_df.shape[0]} time periods\")\n",
    "print(f\"  - {features_df.shape[1]} variables\")\n",
    "print(f\"  - All series are now stationary and ready for time series modeling\")\n",
    "print(f\"\\nUse features_df.dropna() to get a dataset with complete observations.\")\n",
    "print(f\"After dropna(): {len(features_df.dropna())} complete rows available for modeling.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3247611e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create time series plots\n",
    "fig, axes = plt.subplots(4, 2, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, col in enumerate(features_df.columns):\n",
    "    axes[idx].plot(features_df.index, features_df[col], linewidth=1.5, alpha=0.8)\n",
    "    axes[idx].set_title(f'{col}', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Date')\n",
    "    axes[idx].set_ylabel('Value')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "    axes[idx].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('features_timeseries.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Time series plot saved as 'features_timeseries.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec4445b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a clean sample of the data\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"CLEAN DATA SAMPLE (Last 10 rows with complete data)\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Get rows with complete data (no NaN)\n",
    "complete_data = features_df.dropna()\n",
    "if len(complete_data) > 0:\n",
    "    print(f\"\\nTotal rows with NO missing values: {len(complete_data)}\")\n",
    "    print(\"\\nLast 10 rows:\")\n",
    "    with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.width', None):\n",
    "        print(complete_data.tail(10).to_string())\n",
    "else:\n",
    "    print(\"No rows with complete data. Showing most recent available data:\")\n",
    "    print(features_df.tail(10).to_string())\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"DATA READY FOR EXPORT/MODELING\")\n",
    "print(\"=\"*100)\n",
    "print(f\"\\nYou can now use features_df for your time series modeling!\")\n",
    "print(f\"\\nQuick usage examples:\")\n",
    "print(f\"  1. Get clean data: clean_df = features_df.dropna()\")\n",
    "print(f\"  2. Export to CSV: features_df.to_csv('features.csv')\")\n",
    "print(f\"  3. Access specific column: features_df['beefprice']\")\n",
    "print(f\"  4. Get date range: print(features_df.index.min(), 'to', features_df.index.max())\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85744106",
   "metadata": {},
   "source": [
    "## 10. Stationarity Verification\n",
    "\n",
    "Verify that ALL series are now stationary after transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcd726b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*100)\n",
    "print(\"STATIONARITY VERIFICATION - ALL TRANSFORMED SERIES\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "print(\"\\nðŸ“Š ADF TEST RESULTS ON FINAL FEATURES (after transformation):\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "verification_results = []\n",
    "for col in features_df.columns:\n",
    "    series_clean = features_df[col].dropna().values\n",
    "    if len(series_clean) >= 3:\n",
    "        result = adfuller(series_clean, autolag='AIC', regression='c')\n",
    "        adf_stat = result[0]\n",
    "        pval = result[1]\n",
    "        critical_5pct = result[4]['5%']\n",
    "        \n",
    "        is_stationary = \"âœ… YES\" if pval < 0.05 else \"âŒ NO\"\n",
    "        \n",
    "        verification_results.append({\n",
    "            'Variable': col,\n",
    "            'ADF Statistic': adf_stat,\n",
    "            'P-Value': pval,\n",
    "            'Critical (5%)': critical_5pct,\n",
    "            'Stationary?': is_stationary\n",
    "        })\n",
    "\n",
    "verification_df = pd.DataFrame(verification_results)\n",
    "print(verification_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "stationary_count = sum(1 for row in verification_results if row['P-Value'] < 0.05)\n",
    "total_count = len(verification_results)\n",
    "\n",
    "print(f\"\\nâœ… Stationary Series: {stationary_count} / {total_count}\")\n",
    "print(f\"âŒ Non-Stationary Series: {total_count - stationary_count} / {total_count}\")\n",
    "\n",
    "if stationary_count == total_count:\n",
    "    print(f\"\\nðŸŽ‰ SUCCESS! All {total_count} variables are now STATIONARY and ready for modeling!\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸  Warning: {total_count - stationary_count} series still non-stationary\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
