{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b395d28",
   "metadata": {},
   "source": [
    "# ECM Forecasting Performance Evaluation\n",
    "\n",
    "This notebook evaluates the out-of-sample forecasting performance of the Error Correction Model (ECM) for beef prices.\n",
    "\n",
    "**Approach:**\n",
    "- Train ECM on first 200 observations\n",
    "- Test on remaining observations\n",
    "- Assumes cointegration between beef and poultry prices (established in main ECM notebook)\n",
    "- No need to re-run stationarity tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe79f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344d0a17",
   "metadata": {},
   "source": [
    "## 1. Load and Split Data\n",
    "\n",
    "Load both levels (for long-run relationship) and first-differenced data (for ECM), then split into train/test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a5ea2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load clean data (in levels)\n",
    "df_clean = pd.read_csv('../data/df_clean.csv', index_col=0, parse_dates=True)\n",
    "\n",
    "# Drop D0_USA to avoid multicollinearity\n",
    "df_clean = df_clean.drop(columns=['D0_USA'])\n",
    "\n",
    "# Load first differenced data\n",
    "df_firstdiff = pd.read_csv('../data/df_firstdiff.csv', index_col=0, parse_dates=True)\n",
    "\n",
    "# Drop D0_USA to avoid multicollinearity\n",
    "df_firstdiff = df_firstdiff.drop(columns=['D0_USA'])\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"DATA LOADED\")\n",
    "print(\"=\"*100)\n",
    "print(f\"\\nClean data (levels): {df_clean.shape}\")\n",
    "print(f\"First-differenced data: {df_firstdiff.shape}\")\n",
    "print(f\"\\nDate range: {df_clean.index.min()} to {df_clean.index.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1032072b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "n_train = 200\n",
    "\n",
    "# Split levels data\n",
    "df_clean_train = df_clean.iloc[:n_train]\n",
    "df_clean_test = df_clean.iloc[n_train:]\n",
    "\n",
    "# Split first-differenced data\n",
    "df_firstdiff_train = df_firstdiff.iloc[:n_train]\n",
    "df_firstdiff_test = df_firstdiff.iloc[n_train:]\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"TRAIN-TEST SPLIT\")\n",
    "print(\"=\"*100)\n",
    "print(f\"\\nTraining set:\")\n",
    "print(f\"  - Levels data: {df_clean_train.shape[0]} observations | {df_clean_train.index[0]} to {df_clean_train.index[-1]}\")\n",
    "print(f\"  - Differenced data: {df_firstdiff_train.shape[0]} observations | {df_firstdiff_train.index[0]} to {df_firstdiff_train.index[-1]}\")\n",
    "\n",
    "print(f\"\\nTest set:\")\n",
    "print(f\"  - Levels data: {df_clean_test.shape[0]} observations | {df_clean_test.index[0]} to {df_clean_test.index[-1]}\")\n",
    "print(f\"  - Differenced data: {df_firstdiff_test.shape[0]} observations | {df_firstdiff_test.index[0]} to {df_firstdiff_test.index[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ce5512",
   "metadata": {},
   "source": [
    "## 2. Estimate Long-Run Relationship (Training Set)\n",
    "\n",
    "Estimate the cointegrating relationship between beef and poultry prices using only the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb6bdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate long-run relationship on training set\n",
    "y_longrun = df_clean_train['PBEEFUSDM']\n",
    "X_longrun = df_clean_train['PPOULTUSDM']\n",
    "X_lr = sm.add_constant(X_longrun)\n",
    "\n",
    "model_longrun = OLS(y_longrun, X_lr).fit()\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"STEP 1: LONG-RUN COINTEGRATING RELATIONSHIP (TRAINING SET)\")\n",
    "print(\"=\"*100)\n",
    "print(f\"\\nBPEEFUSDM = {model_longrun.params['const']:.4f} + {model_longrun.params['PPOULTUSDM']:.4f} √ó PPOULTUSDM\")\n",
    "print(f\"\\nR-squared: {model_longrun.rsquared:.4f}\")\n",
    "print(f\"Observations: {len(y_longrun)}\")\n",
    "\n",
    "# Extract coefficients\n",
    "alpha = model_longrun.params['const']\n",
    "beta = model_longrun.params['PPOULTUSDM']\n",
    "\n",
    "# Calculate equilibrium errors for TRAINING SET\n",
    "equilibrium_error_train = model_longrun.resid\n",
    "print(f\"\\nEquilibrium error calculated for training period\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681b8189",
   "metadata": {},
   "source": [
    "## 3. Estimate ECM (Training Set)\n",
    "\n",
    "Estimate the Error Correction Model using the training data, incorporating the lagged equilibrium error from Step 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1766daf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare ECM variables (training set)\n",
    "y_ecm_train = df_firstdiff_train['PBEEFUSDM']\n",
    "\n",
    "# Lagged equilibrium error\n",
    "u_lag_train = equilibrium_error_train.shift(1)\n",
    "u_lag_train.name = 'u_hat_lag1'\n",
    "\n",
    "# Short-run variables (based on main ECM notebook specification)\n",
    "shortrun_vars = [\n",
    "    'PPOULTUSDM',\n",
    "    'PLAMBUSDM',\n",
    "    'PPORKUSDM',\n",
    "    'POILBREUSDM',\n",
    "    'fao_food_index',\n",
    "    'retail_china',\n",
    "    'fnbretail_USA',\n",
    "    'bioethanol_production',\n",
    "    'D1_USA',\n",
    "    'D2_USA',\n",
    "    'D3_USA',\n",
    "    'D4_USA',\n",
    "    'precip_BR',\n",
    "    'enso_anomaly'\n",
    "]\n",
    "\n",
    "# Lagged dependent variable\n",
    "beef_lag1_train = df_firstdiff_train['PBEEFUSDM'].shift(1)\n",
    "beef_lag1_train.name = 'PBEEFUSDM_lag1'\n",
    "\n",
    "# Combine all regressors\n",
    "X_ecm_components = [u_lag_train] + [df_firstdiff_train[var] for var in shortrun_vars] + [beef_lag1_train]\n",
    "X_ecm_train = pd.concat(X_ecm_components, axis=1)\n",
    "X_ecm_train = sm.add_constant(X_ecm_train)\n",
    "\n",
    "# Align and clean\n",
    "data_ecm_train = pd.concat([y_ecm_train, X_ecm_train], axis=1).dropna()\n",
    "y_ecm_train_clean = data_ecm_train.iloc[:, 0]\n",
    "X_ecm_train_clean = data_ecm_train.iloc[:, 1:]\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"STEP 2: ERROR CORRECTION MODEL (TRAINING SET)\")\n",
    "print(\"=\"*100)\n",
    "print(f\"\\nECM training sample: {len(y_ecm_train_clean)} observations\")\n",
    "print(f\"Period: {data_ecm_train.index[0]} to {data_ecm_train.index[-1]}\")\n",
    "print(f\"Number of regressors: {X_ecm_train_clean.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205c726e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate ECM with HAC standard errors\n",
    "maxlags_hac = int(4 * (len(y_ecm_train_clean) / 100) ** (2/9))\n",
    "model_ecm = OLS(y_ecm_train_clean, X_ecm_train_clean).fit(cov_type='HAC', cov_kwds={'maxlags': maxlags_hac})\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"ECM ESTIMATION RESULTS (HAC STANDARD ERRORS)\")\n",
    "print(\"=\"*100)\n",
    "print(model_ecm.summary())\n",
    "\n",
    "# Extract error correction coefficient\n",
    "phi = model_ecm.params['u_hat_lag1']\n",
    "print(f\"\\n‚úì Error correction coefficient (œÜ): {phi:.6f}\")\n",
    "print(f\"  p-value: {model_ecm.pvalues['u_hat_lag1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16a9cad",
   "metadata": {},
   "source": [
    "## 4. Generate Forecasts for Test Set\n",
    "\n",
    "Use the trained ECM to forecast beef price changes in the test period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c27100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate equilibrium errors for the FULL SAMPLE (needed for test set forecasting)\n",
    "# Using the long-run coefficients estimated from training set\n",
    "equilibrium_error_full = df_clean['PBEEFUSDM'] - (alpha + beta * df_clean['PPOULTUSDM'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"GENERATING TEST SET FORECASTS\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Prepare test set features\n",
    "y_ecm_test = df_firstdiff_test['PBEEFUSDM']\n",
    "\n",
    "# Lagged equilibrium error for test set\n",
    "u_lag_test = equilibrium_error_full.shift(1).loc[df_firstdiff_test.index]\n",
    "u_lag_test.name = 'u_hat_lag1'\n",
    "\n",
    "# Lagged dependent variable for test set\n",
    "beef_lag1_test = df_firstdiff['PBEEFUSDM'].shift(1).loc[df_firstdiff_test.index]\n",
    "beef_lag1_test.name = 'PBEEFUSDM_lag1'\n",
    "\n",
    "# Combine test set regressors\n",
    "X_ecm_test_components = [u_lag_test] + [df_firstdiff_test[var] for var in shortrun_vars] + [beef_lag1_test]\n",
    "X_ecm_test = pd.concat(X_ecm_test_components, axis=1)\n",
    "X_ecm_test = sm.add_constant(X_ecm_test)\n",
    "\n",
    "# Align test data\n",
    "data_ecm_test = pd.concat([y_ecm_test, X_ecm_test], axis=1).dropna()\n",
    "y_ecm_test_clean = data_ecm_test.iloc[:, 0]\n",
    "X_ecm_test_clean = data_ecm_test.iloc[:, 1:]\n",
    "\n",
    "print(f\"\\nTest set observations: {len(y_ecm_test_clean)}\")\n",
    "print(f\"Period: {data_ecm_test.index[0]} to {data_ecm_test.index[-1]}\")\n",
    "\n",
    "# Generate predictions\n",
    "y_pred_test = model_ecm.predict(X_ecm_test_clean)\n",
    "\n",
    "print(f\"\\n‚úì Forecasts generated for {len(y_pred_test)} test observations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0fdcb7",
   "metadata": {},
   "source": [
    "## 5. Evaluate Forecast Performance\n",
    "\n",
    "Calculate forecast accuracy metrics and compare predictions with actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da67930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate forecast metrics\n",
    "rmse = np.sqrt(mean_squared_error(y_ecm_test_clean, y_pred_test))\n",
    "mae = mean_absolute_error(y_ecm_test_clean, y_pred_test)\n",
    "r2 = r2_score(y_ecm_test_clean, y_pred_test)\n",
    "mape = np.mean(np.abs((y_ecm_test_clean - y_pred_test) / y_ecm_test_clean)) * 100\n",
    "\n",
    "# Calculate directional accuracy (sign prediction)\n",
    "actual_direction = np.sign(y_ecm_test_clean)\n",
    "pred_direction = np.sign(y_pred_test)\n",
    "directional_accuracy = np.mean(actual_direction == pred_direction) * 100\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"FORECAST PERFORMANCE METRICS\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "print(\"\\nüìä ERROR METRICS (Predicting Monthly Changes)\")\n",
    "print(\"-\"*100)\n",
    "print(f\"RMSE (Root Mean Squared Error):  {rmse:.6f}\")\n",
    "print(f\"MAE (Mean Absolute Error):        {mae:.6f}\")\n",
    "print(f\"MAPE (Mean Absolute % Error):     {mape:.2f}%\")\n",
    "print(f\"R¬≤ (Out-of-sample):               {r2:.4f}\")\n",
    "\n",
    "print(\"\\nüìà DIRECTIONAL ACCURACY\")\n",
    "print(\"-\"*100)\n",
    "print(f\"Correct direction predictions:    {directional_accuracy:.2f}%\")\n",
    "print(f\"(Percentage of times model correctly predicts sign of price change)\")\n",
    "\n",
    "# Baseline comparison (naive forecast: no change)\n",
    "naive_rmse = np.sqrt(mean_squared_error(y_ecm_test_clean, np.zeros(len(y_ecm_test_clean))))\n",
    "naive_mae = mean_absolute_error(y_ecm_test_clean, np.zeros(len(y_ecm_test_clean)))\n",
    "\n",
    "print(\"\\nüéØ COMPARISON WITH NAIVE FORECAST (No Change Prediction)\")\n",
    "print(\"-\"*100)\n",
    "print(f\"Naive RMSE:                       {naive_rmse:.6f}\")\n",
    "print(f\"ECM RMSE:                         {rmse:.6f}\")\n",
    "print(f\"RMSE Improvement:                 {((naive_rmse - rmse) / naive_rmse * 100):.2f}%\")\n",
    "print(f\"\\nNaive MAE:                        {naive_mae:.6f}\")\n",
    "print(f\"ECM MAE:                          {mae:.6f}\")\n",
    "print(f\"MAE Improvement:                  {((naive_mae - mae) / naive_mae * 100):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f830e52",
   "metadata": {},
   "source": [
    "## 6. Visualize Forecast Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb95c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive forecast visualization\n",
    "fig, axes = plt.subplots(3, 1, figsize=(15, 12))\n",
    "\n",
    "# Plot 1: Actual vs Predicted (Time Series)\n",
    "ax1 = axes[0]\n",
    "ax1.plot(y_ecm_test_clean.index, y_ecm_test_clean.values, \n",
    "         label='Actual', linewidth=2, marker='o', markersize=4)\n",
    "ax1.plot(y_pred_test.index, y_pred_test.values, \n",
    "         label='Predicted', linewidth=2, marker='s', markersize=4, alpha=0.7)\n",
    "ax1.axhline(y=0, color='black', linestyle='--', linewidth=1, alpha=0.5)\n",
    "ax1.set_xlabel('Date', fontsize=11)\n",
    "ax1.set_ylabel('Beef Price Change (First Difference)', fontsize=11)\n",
    "ax1.set_title('ECM Forecast Performance: Actual vs Predicted Price Changes', \n",
    "              fontsize=13, fontweight='bold')\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Forecast Errors\n",
    "ax2 = axes[1]\n",
    "errors = y_ecm_test_clean - y_pred_test\n",
    "ax2.plot(errors.index, errors.values, color='red', linewidth=1.5, marker='o', markersize=4)\n",
    "ax2.axhline(y=0, color='black', linestyle='--', linewidth=1)\n",
    "ax2.fill_between(errors.index, errors.values, 0, alpha=0.3, color='red')\n",
    "ax2.set_xlabel('Date', fontsize=11)\n",
    "ax2.set_ylabel('Forecast Error', fontsize=11)\n",
    "ax2.set_title(f'Forecast Errors (RMSE: {rmse:.6f}, MAE: {mae:.6f})', \n",
    "              fontsize=13, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Actual vs Predicted (Scatter)\n",
    "ax3 = axes[2]\n",
    "ax3.scatter(y_ecm_test_clean, y_pred_test, alpha=0.6, s=50)\n",
    "# 45-degree line\n",
    "min_val = min(y_ecm_test_clean.min(), y_pred_test.min())\n",
    "max_val = max(y_ecm_test_clean.max(), y_pred_test.max())\n",
    "ax3.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "ax3.set_xlabel('Actual Price Change', fontsize=11)\n",
    "ax3.set_ylabel('Predicted Price Change', fontsize=11)\n",
    "ax3.set_title(f'Actual vs Predicted Scatter (R¬≤ = {r2:.4f})', \n",
    "              fontsize=13, fontweight='bold')\n",
    "ax3.legend(fontsize=10)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d931a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual diagnostics plot\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Residual histogram\n",
    "ax1 = axes[0, 0]\n",
    "ax1.hist(errors, bins=20, edgecolor='black', alpha=0.7)\n",
    "ax1.axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "ax1.set_xlabel('Forecast Error', fontsize=10)\n",
    "ax1.set_ylabel('Frequency', fontsize=10)\n",
    "ax1.set_title('Distribution of Forecast Errors', fontsize=11, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Residual Q-Q plot\n",
    "from scipy import stats as sp_stats\n",
    "ax2 = axes[0, 1]\n",
    "sp_stats.probplot(errors, dist=\"norm\", plot=ax2)\n",
    "ax2.set_title('Q-Q Plot of Forecast Errors', fontsize=11, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Cumulative forecast error\n",
    "ax3 = axes[1, 0]\n",
    "cumulative_error = errors.cumsum()\n",
    "ax3.plot(cumulative_error.index, cumulative_error.values, linewidth=2, color='purple')\n",
    "ax3.axhline(y=0, color='black', linestyle='--', linewidth=1)\n",
    "ax3.set_xlabel('Date', fontsize=10)\n",
    "ax3.set_ylabel('Cumulative Forecast Error', fontsize=10)\n",
    "ax3.set_title('Cumulative Forecast Error Over Time', fontsize=11, fontweight='bold')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Absolute errors over time\n",
    "ax4 = axes[1, 1]\n",
    "abs_errors = np.abs(errors)\n",
    "ax4.plot(abs_errors.index, abs_errors.values, linewidth=1.5, color='orange', marker='o', markersize=4)\n",
    "ax4.axhline(y=mae, color='red', linestyle='--', linewidth=2, label=f'MAE = {mae:.6f}')\n",
    "ax4.set_xlabel('Date', fontsize=10)\n",
    "ax4.set_ylabel('Absolute Forecast Error', fontsize=10)\n",
    "ax4.set_title('Absolute Forecast Errors', fontsize=11, fontweight='bold')\n",
    "ax4.legend(fontsize=9)\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e23488a",
   "metadata": {},
   "source": [
    "## 7. Summary and Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc9321e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"ECM FORECAST EVALUATION SUMMARY\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "print(\"\\nüìã MODEL SPECIFICATION\")\n",
    "print(\"-\"*100)\n",
    "print(f\"Training period: {df_clean_train.index[0]} to {df_clean_train.index[-1]} ({n_train} obs)\")\n",
    "print(f\"Test period: {df_clean_test.index[0]} to {df_clean_test.index[-1]} ({len(df_clean_test)} obs)\")\n",
    "print(f\"\\nLong-run relationship: BEEF = {alpha:.4f} + {beta:.4f} √ó POULTRY\")\n",
    "print(f\"Error correction coefficient: œÜ = {phi:.6f}\")\n",
    "\n",
    "print(\"\\nüìä FORECAST PERFORMANCE\")\n",
    "print(\"-\"*100)\n",
    "print(f\"Out-of-sample R¬≤: {r2:.4f}\")\n",
    "print(f\"RMSE: {rmse:.6f}\")\n",
    "print(f\"MAE: {mae:.6f}\")\n",
    "print(f\"Directional accuracy: {directional_accuracy:.2f}%\")\n",
    "\n",
    "print(\"\\nüí° INTERPRETATION\")\n",
    "print(\"-\"*100)\n",
    "\n",
    "if r2 > 0:\n",
    "    print(f\"‚úì Positive out-of-sample R¬≤ ({r2:.4f}) indicates the ECM has predictive power\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Negative out-of-sample R¬≤ ({r2:.4f}) suggests limited predictive power\")\n",
    "\n",
    "if directional_accuracy > 50:\n",
    "    print(f\"‚úì Directional accuracy ({directional_accuracy:.1f}%) exceeds random chance (50%)\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Directional accuracy ({directional_accuracy:.1f}%) does not exceed random chance\")\n",
    "\n",
    "improvement = ((naive_rmse - rmse) / naive_rmse * 100)\n",
    "if improvement > 0:\n",
    "    print(f\"‚úì ECM outperforms naive forecast by {improvement:.1f}% (RMSE reduction)\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è ECM does not outperform naive forecast (RMSE higher by {abs(improvement):.1f}%)\")\n",
    "\n",
    "print(\"\\nüîç NOTES\")\n",
    "print(\"-\"*100)\n",
    "print(\"‚Ä¢ Forecasting monthly commodity price changes is inherently difficult\")\n",
    "print(\"‚Ä¢ Low R¬≤ is common for short-term price forecasts due to market noise\")\n",
    "print(\"‚Ä¢ ECM captures long-run equilibrium relationship, not short-term volatility\")\n",
    "print(\"‚Ä¢ Directional accuracy may be more relevant than point forecast accuracy\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"END OF FORECAST EVALUATION\")\n",
    "print(\"=\"*100)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
